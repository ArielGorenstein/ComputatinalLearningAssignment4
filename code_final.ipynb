{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66d2dVi8b2QL",
        "outputId": "1e37f632-ec57-4b39-d47a-004619b299cf"
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.7/dist-packages (1.4.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49BfbAKYxpTv"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Softmax\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "import xlsxwriter\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NzcPGDqh-ph"
      },
      "source": [
        "def read_csv(path):\n",
        "    all = pd.read_csv(path, sep=',', engine='python', quoting=csv.QUOTE_NONE)\n",
        "    X, y = all.drop(all.columns[-1], 1).to_numpy(), all[all.columns[-1]].to_numpy()\n",
        "    return X, y"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut0_-ERukY71"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, num_classes, epochs=3, lr=0.001):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.num_classes = num_classes\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
        "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
        "        self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        out = F.softmax(out, dim=0)\n",
        "        return out\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        tx = torch.tensor(x).float()\n",
        "        ty = torch.tensor(y)\n",
        "        dataset = TensorDataset(tx, ty)\n",
        "        data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            optimizer.zero_grad()\n",
        "            for data in data_loader:  # `data` is a batch of data\n",
        "                X, y = data  # X is the batch of features, y is the batch of targets.\n",
        "                self.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
        "                output = self(X.view(-1, self.input_size))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
        "                loss = loss_function(output, y)  # calc and grab the loss value\n",
        "                loss.backward()  # apply this loss backwards thru the network's parameters\n",
        "                optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
        "\n",
        "    def predict(self, x):\n",
        "        ans = []\n",
        "        output = self(torch.tensor(x).view(-1, self.input_size).float())\n",
        "        for idx, i in enumerate(output):\n",
        "            ans.append(torch.argmax(i).item())\n",
        "        return ans\n",
        "\n",
        "    def predict_proba(self, x):\n",
        "        ans = []\n",
        "        output = self(torch.tensor(x).view(-1, self.input_size).float())\n",
        "        for idx, i in enumerate(output):\n",
        "            row = i.tolist()\n",
        "            ans.append(row if sum(row) == 1 else [float(i)/sum(row) for i in row])\n",
        "        return ans\n",
        "\n",
        "    def get_params(self, deep):\n",
        "      return {\n",
        "          'input_size': self.input_size, \n",
        "          'hidden1_size': self.hidden1_size, \n",
        "          'hidden2_size': self.hidden2_size, \n",
        "          'num_classes': self.num_classes, \n",
        "          'epochs': self.epochs,\n",
        "          'lr': self.lr\n",
        "      }\n",
        "    \n",
        "    def set_params(self, **parameters):\n",
        "      for parameter, value in parameters.items():\n",
        "          setattr(self, parameter, value)\n",
        "      return self\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svg7snnK14jJ"
      },
      "source": [
        "class NoiseNet(Net):\n",
        "  \n",
        "  def add_noise(self, X):\n",
        "    for j in range(len(X[0])):\n",
        "      std = np.std([x[j] for x in X])\n",
        "      for i in range(len(X)):\n",
        "        X[i][j] = X[i][j] + np.random.normal(0, std / 4)\n",
        "    return X\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    Net.fit(self, self.add_noise(X), y)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqRQ9i2cxHJf"
      },
      "source": [
        "class AdvancedNoise(Net):\n",
        "\n",
        "  def add_noise(self, X):\n",
        "    iso = IsolationForest(contamination=0.1)\n",
        "    yhat = iso.fit_predict(X)\n",
        "    for j in range(len(X[0])):\n",
        "      std = np.std([x[j] for x in X])\n",
        "      for i in range(len(X)):\n",
        "        if yhat[i] == 1:\n",
        "          X[i][j] = X[i][j] + np.random.normal(0, std / 4)\n",
        "    return X\n",
        "    \n",
        "\n",
        "  def fit(self, X, y):\n",
        "    Net.fit(self, self.add_noise(X), y)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J_2HB4irYG1"
      },
      "source": [
        "def evaluate(y_pred_p, y_test):\n",
        "    y_pred = [pred_p.index(max(pred_p)) for pred_p in y_pred_p]\n",
        "    precision = precision_score(y_test, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_test, y_pred, average='macro', zero_division=1)\n",
        "    accuracy = accuracy_score(y_test, y_pred) \n",
        "    fpr = (np.mean([len([_ for (p, t) in list(zip(y_pred, y_test)) if p == cl and t != cl]) / len([t for t in y_test if t != cl]) for cl in np.unique(y_test)])) if len(np.unique(y_test)) > 1 else 0\n",
        "    auc_score = (roc_auc_score(y_test, y_pred_p, multi_class='ovr') if len(np.unique(y_test)) > 2 else roc_auc_score(y_test, np.array(y_pred_p)[:, 1])) if len(np.unique(y_test)) > 1 else 1\n",
        "    return {\n",
        "        'precision': round(precision, 3),\n",
        "        'tpr': round(recall, 3),\n",
        "        'accuracy': round(accuracy, 3),\n",
        "        'fpr': round(fpr, 3),\n",
        "        'auc': round(auc_score, 3)\n",
        "    }"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elIP2mJ8x6ZO"
      },
      "source": [
        "def inner_cross_validation(algorithm, distributions, X, y):\n",
        "  clf = RandomizedSearchCV(algorithm, distributions, n_iter=50, cv=3, scoring='accuracy')\n",
        "  search = clf.fit(X, y)\n",
        "  return search.best_estimator_, search.best_params_, round(np.mean(search.cv_results_['mean_fit_time']), 3), round(np.mean(search.cv_results_['mean_score_time']), 3)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjfB3o4PAzNL"
      },
      "source": [
        "def outer_cross_validation(algorithm, distributions, X, y, folds=10):\n",
        "  kf = KFold(n_splits=folds)\n",
        "  cycle = 0\n",
        "  results = []\n",
        "  for train_index, test_index in kf.split(X):\n",
        "    cycle = cycle + 1\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    best_model, best_params, train_time, score_time = inner_cross_validation(algorithm, distributions, X_train, y_train)\n",
        "    y_pred_p = best_model.predict_proba(X_test)\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    evaluation = {**{'cycle': cycle, 'params': str(best_params), 'train time': train_time, 'score time': score_time}, \n",
        "                  **evaluate(y_pred_p, y_test)}\n",
        "    print(evaluation)\n",
        "    results.append(evaluation)\n",
        "  return results"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMMzfExgCGaR",
        "outputId": "759250db-718a-4a8c-d51c-c36cdf5e573d"
      },
      "source": [
        "def write_excel(workbook_name, data):\n",
        "  workbook = xlsxwriter.Workbook(workbook_name)\n",
        "  worksheet = workbook.add_worksheet()\n",
        "  for i in range(len(data)):\n",
        "    for j in range(len(data[i])):\n",
        "      worksheet.write(i, j, data[i][j])\n",
        "  workbook.close()\n",
        "\n",
        "\n",
        "all_results = [['dataset', 'algorithm', 'cv', 'hyper parameters', 'accuracy', 'tpr', 'fpr', 'precision', 'auc', 'train time', 'inference time']]\n",
        "for f in os.listdir('datasets'):\n",
        "  print(f\"******** current file: {f} ********\")\n",
        "  if os.path.splitext(f'datasets/{f}')[1] != '.csv':\n",
        "    continue\n",
        "  try:\n",
        "    X, y = read_csv(f\"datasets/{f}\")\n",
        "    input_size = len(X[0])\n",
        "    num_classes = len(set(y))\n",
        "    algorithms = [\n",
        "        {\n",
        "          'name': 'FNN + Advanced Noise',\n",
        "          'alg': AdvancedNoise(input_size, 64, 3, num_classes),\n",
        "          'distributions': {\n",
        "              'epochs': [3, 4, 5, 6, 7],\n",
        "              'lr': [0.0005, 0.001, 0.0015, 0.002, 0.0025, 0.003, 0.0035, 0.004, 0.0045, 0.005]\n",
        "        }\n",
        "        },\n",
        "        {\n",
        "          'name': 'FNN + Noise',\n",
        "          'alg': NoiseNet(input_size, 64, 3, num_classes),\n",
        "          'distributions': {\n",
        "              'epochs': [3, 4, 5, 6, 7],\n",
        "              'lr': [0.0005, 0.001, 0.0015, 0.002, 0.0025, 0.003, 0.0035, 0.004, 0.0045, 0.005]\n",
        "        }\n",
        "        },\n",
        "        {\n",
        "          'name': 'FNN',\n",
        "          'alg': Net(input_size, 64, 3, num_classes),\n",
        "          'distributions': {\n",
        "              'epochs': [3, 4, 5, 6, 7],\n",
        "              'lr': [0.0005, 0.001, 0.0015, 0.002, 0.0025, 0.003, 0.0035, 0.004, 0.0045, 0.005]\n",
        "          }\n",
        "        }\n",
        "    ]\n",
        "    for algorithm in algorithms:\n",
        "      print(\"-\"*80)\n",
        "      print(\" \"*30 + algorithm['name'] + \" \"*30)\n",
        "      print(\"-\"*80)\n",
        "      results = outer_cross_validation(algorithm['alg'], algorithm['distributions'], X, y)\n",
        "      for r in results:\n",
        "        all_results = all_results + [[f, algorithm['name'], r['cycle'], r['params'], r['accuracy'], r['tpr'], r['fpr'], r['precision'], r['auc'], r['train time'], r['score time']]]\n",
        "  except Exception as e:\n",
        "    print(f\">>>>>>>>>>>>>>>>>> something inevitably went wrong: {e} <<<<<<<<<<<<<<<<<<<<<\\n\")\n",
        "\n",
        "write_excel('results.xls', all_results)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** current file: .ipynb_checkpoints ********\n",
            "******** current file: parkinsons.csv ********\n",
            "--------------------------------------------------------------------------------\n",
            "                              FNN + Advanced Noise                              \n",
            "--------------------------------------------------------------------------------\n",
            "{'cycle': 1, 'params': \"{'lr': 0.002, 'epochs': 4}\", 'train time': 0.282, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.825, 'accuracy': 0.65, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 2, 'params': \"{'lr': 0.0025, 'epochs': 6}\", 'train time': 0.282, 'score time': 0.001, 'precision': 0.714, 'tpr': 0.714, 'accuracy': 0.6, 'fpr': 0.286, 'auc': 1.0}\n",
            "{'cycle': 3, 'params': \"{'lr': 0.004, 'epochs': 7}\", 'train time': 0.28, 'score time': 0.001, 'precision': 0.698, 'tpr': 0.688, 'accuracy': 0.65, 'fpr': 0.312, 'auc': 0.885}\n",
            "{'cycle': 4, 'params': \"{'lr': 0.002, 'epochs': 7}\", 'train time': 0.282, 'score time': 0.001, 'precision': 1.0, 'tpr': 1.0, 'accuracy': 1.0, 'fpr': 0.0, 'auc': 1.0}\n",
            "{'cycle': 5, 'params': \"{'lr': 0.003, 'epochs': 7}\", 'train time': 0.28, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.85, 'accuracy': 0.7, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 6, 'params': \"{'lr': 0.0035, 'epochs': 6}\", 'train time': 0.283, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.579, 'accuracy': 0.158, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 7, 'params': \"{'lr': 0.005, 'epochs': 7}\", 'train time': 0.281, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.684, 'accuracy': 0.368, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 8, 'params': \"{'lr': 0.003, 'epochs': 4}\", 'train time': 0.283, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.605, 'accuracy': 0.211, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 9, 'params': \"{'lr': 0.0025, 'epochs': 5}\", 'train time': 0.282, 'score time': 0.001, 'precision': 0.786, 'tpr': 0.727, 'accuracy': 0.684, 'fpr': 0.273, 'auc': 0.989}\n",
            "{'cycle': 10, 'params': \"{'lr': 0.0045, 'epochs': 7}\", 'train time': 0.281, 'score time': 0.001, 'precision': 0.378, 'tpr': 0.359, 'accuracy': 0.368, 'fpr': 0.641, 'auc': 0.308}\n",
            "--------------------------------------------------------------------------------\n",
            "                              FNN + Noise                              \n",
            "--------------------------------------------------------------------------------\n",
            "{'cycle': 1, 'params': \"{'lr': 0.001, 'epochs': 3}\", 'train time': 0.071, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.825, 'accuracy': 0.65, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 2, 'params': \"{'lr': 0.005, 'epochs': 4}\", 'train time': 0.07, 'score time': 0.001, 'precision': 0.8, 'tpr': 0.857, 'accuracy': 0.8, 'fpr': 0.143, 'auc': 1.0}\n",
            "{'cycle': 3, 'params': \"{'lr': 0.005, 'epochs': 5}\", 'train time': 0.07, 'score time': 0.001, 'precision': 0.625, 'tpr': 0.625, 'accuracy': 0.6, 'fpr': 0.375, 'auc': 0.875}\n",
            "{'cycle': 4, 'params': \"{'lr': 0.002, 'epochs': 4}\", 'train time': 0.073, 'score time': 0.001, 'precision': 0.773, 'tpr': 0.821, 'accuracy': 0.75, 'fpr': 0.179, 'auc': 1.0}\n",
            "{'cycle': 5, 'params': \"{'lr': 0.0025, 'epochs': 4}\", 'train time': 0.071, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.85, 'accuracy': 0.7, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 6, 'params': \"{'lr': 0.003, 'epochs': 6}\", 'train time': 0.07, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.737, 'accuracy': 0.474, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 7, 'params': \"{'lr': 0.0035, 'epochs': 4}\", 'train time': 0.072, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.632, 'accuracy': 0.263, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 8, 'params': \"{'lr': 0.0015, 'epochs': 5}\", 'train time': 0.07, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.763, 'accuracy': 0.526, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 9, 'params': \"{'lr': 0.004, 'epochs': 4}\", 'train time': 0.071, 'score time': 0.001, 'precision': 0.786, 'tpr': 0.727, 'accuracy': 0.684, 'fpr': 0.273, 'auc': 0.989}\n",
            "{'cycle': 10, 'params': \"{'lr': 0.0035, 'epochs': 5}\", 'train time': 0.072, 'score time': 0.001, 'precision': 0.449, 'tpr': 0.442, 'accuracy': 0.421, 'fpr': 0.558, 'auc': 0.423}\n",
            "--------------------------------------------------------------------------------\n",
            "                              FNN                              \n",
            "--------------------------------------------------------------------------------\n",
            "{'cycle': 1, 'params': \"{'lr': 0.0035, 'epochs': 7}\", 'train time': 0.055, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.825, 'accuracy': 0.65, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 2, 'params': \"{'lr': 0.003, 'epochs': 4}\", 'train time': 0.055, 'score time': 0.001, 'precision': 0.7, 'tpr': 0.679, 'accuracy': 0.55, 'fpr': 0.321, 'auc': 0.75}\n",
            "{'cycle': 3, 'params': \"{'lr': 0.0045, 'epochs': 6}\", 'train time': 0.055, 'score time': 0.001, 'precision': 0.786, 'tpr': 0.75, 'accuracy': 0.7, 'fpr': 0.25, 'auc': 0.875}\n",
            "{'cycle': 4, 'params': \"{'lr': 0.004, 'epochs': 4}\", 'train time': 0.055, 'score time': 0.001, 'precision': 1.0, 'tpr': 1.0, 'accuracy': 1.0, 'fpr': 0.0, 'auc': 1.0}\n",
            "{'cycle': 5, 'params': \"{'lr': 0.0035, 'epochs': 3}\", 'train time': 0.055, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.775, 'accuracy': 0.55, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 6, 'params': \"{'lr': 0.004, 'epochs': 6}\", 'train time': 0.055, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.737, 'accuracy': 0.474, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 7, 'params': \"{'lr': 0.0025, 'epochs': 6}\", 'train time': 0.056, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.684, 'accuracy': 0.368, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 8, 'params': \"{'lr': 0.003, 'epochs': 3}\", 'train time': 0.054, 'score time': 0.001, 'precision': 0.5, 'tpr': 0.658, 'accuracy': 0.316, 'fpr': 0, 'auc': 1}\n",
            "{'cycle': 9, 'params': \"{'lr': 0.004, 'epochs': 7}\", 'train time': 0.055, 'score time': 0.001, 'precision': 0.786, 'tpr': 0.727, 'accuracy': 0.684, 'fpr': 0.273, 'auc': 0.875}\n",
            "{'cycle': 10, 'params': \"{'lr': 0.005, 'epochs': 5}\", 'train time': 0.055, 'score time': 0.001, 'precision': 0.411, 'tpr': 0.397, 'accuracy': 0.421, 'fpr': 0.603, 'auc': 0.346}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5S4avvoOwS2",
        "outputId": "1b43223f-32c7-4ea2-da41-5c93b196af52"
      },
      "source": [
        "import xlrd\n",
        "\n",
        "def read_excel():\n",
        "    loc = \"results.xls\"\n",
        "    wb = xlrd.open_workbook(loc)\n",
        "    sheet = wb.sheet_by_index(0)\n",
        "    ans = []\n",
        "    for i in range(1, sheet.nrows):\n",
        "        current = []\n",
        "        for j in range(sheet.ncols):\n",
        "            current.append(sheet.cell_value(i, j))\n",
        "        ans.append(current)\n",
        "    return ans\n",
        "\n",
        "def calculate_chi_score():\n",
        "  accuracy_column = 4\n",
        "  dataset_column = 0\n",
        "  algorithm_column = 1\n",
        "  all_results = read_excel()\n",
        "  all_datasets = list(np.unique([r[dataset_column] for r in all_results]))\n",
        "  all_algorithms = list(np.unique([r[algorithm_column] for r in all_results]))\n",
        "\n",
        "  alg_all_ranks = {alg: [] for alg in all_algorithms}\n",
        "  for dataset in all_datasets:\n",
        "      alg_results = {}\n",
        "      for alg in all_algorithms:\n",
        "          result = np.mean([r[accuracy_column] for r in all_results if r[dataset_column] == dataset and r[algorithm_column] == alg])\n",
        "          alg_results[alg] = result\n",
        "      relative_order = [k for k, v in sorted(alg_results.items(), key=lambda item: item[1])]\n",
        "      for i in range(len(relative_order)):\n",
        "          alg_all_ranks[relative_order[i]].append(i + 1)\n",
        "\n",
        "  alg_mean_ranks = {alg: np.mean(ranks) for alg, ranks in alg_all_ranks.items()}\n",
        "  Rj = alg_mean_ranks.values()\n",
        "  N = 20\n",
        "  L = 3\n",
        "  x_formula = 12 * N / (L * (L + 1)) * sum([(rj - ((L + 1) / 2))**2 for rj in Rj])\n",
        "  print(alg_mean_ranks)\n",
        "  print(x_formula)\n",
        "\n",
        "\n",
        "calculate_chi_score()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'FNN': 2.45, 'FNN + Advanced Noise': 1.95, 'FNN + Noise': 1.6}\n",
            "7.3000000000000025\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}